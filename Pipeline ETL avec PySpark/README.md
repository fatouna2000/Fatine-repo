## üõ†Ô∏è Projet de Formation : Pipeline ETL avec PySpark
## üìã Contexte
Ce projet a √©t√© r√©alis√© dans le cadre de la formation Data Engineering d'IBM pour d√©montrer les comp√©tences acquises en traitement de donn√©es √† grande √©chelle avec PySpark.

## üéØ Objectifs du Projet
Chargement : T√©l√©chargement automatique de datasets depuis le cloud

Transformation : Nettoyage, enrichissement et jointure de donn√©es

Agr√©gation : Calculs de totaux et moyennes par diff√©rentes dimensions

Stockage : Persistance dans Hive et HDFS pour analyse future

## üèóÔ∏è Technologies Utilis√©es
PySpark 3.4.4 - Traitement distribu√©

Python 3.7 - Langage principal

Hive - Entrep√¥t de donn√©es

HDFS - Syst√®me de fichiers distribu√©

Jupyter Notebook - Environnement de d√©veloppement

## üìä Fonctionnalit√©s Cl√©s
ETL Complet : De l'extraction au chargement

Jointures Intelligentes : Sur la colonne customer_id

Enrichissement Temporel : Ajout de colonnes year et quarter

Filtrage Business : Transactions > 1000

Agr√©gations Multi-niveaux : Par client, trimestre et ann√©e

Stockage Optimis√© : Tables Hive + fichiers Parquet HDFS

## üöÄ R√©sultats Concrets
2 tables Hive cr√©√©es : customer_totals et quarterly_averages

1 dataset Parquet dans HDFS : filtered_data.parquet

12 transformations appliqu√©es automatiquement

Pipeline reproductible pour traitement √† grande √©chelle

## üß† Comp√©tences IBM D√©velopp√©es
Ce projet valide les comp√©tences cl√©s de la formation Data Engineering IBM :

Big Data Processing avec PySpark

Data Warehousing avec Hive

Distributed Storage avec HDFS

ETL Pipeline Development

Data Quality Management



Formation : IBM Data Engineering
Objectif : Validation pratique des comp√©tences PySpark/ETL