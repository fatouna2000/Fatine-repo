from airflow import DAG
from airflow.operators.bash import BashOperator
from datetime import datetime, timedelta

default_args = {
    'owner': 'etl_admin',
    'start_date': datetime(2026, 1, 1),
    'email': ['admin@example.com'],
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5)
}

dag = DAG(
    'process_web_log',
    default_args=default_args,
    description='Pipeline ETL pour analyser les logs web',
    schedule_interval='@daily',
    catchup=False
)

# TÃ¢che d'extraction - VERSION GARANTIE
extract_data = BashOperator(
    task_id='extract_data',
    bash_command="""
       
        # 1. Utiliser un rÃ©pertoire avec garantie de permissions
        TMP_DIR="/tmp/airflow_etl_$$"
        mkdir -p "$TMP_DIR"
        cd "$TMP_DIR"
        
        # 2. Copier le fichier source
        cp /home/project/airflow/dags/capstone/accesslog.txt .
        
        # 3. ExÃ©cuter l'extraction
        cut -d" " -f1 accesslog.txt > extracted_data.txt
        
        # 4. Copier le rÃ©sultat vers le rÃ©pertoire final
        cp extracted_data.txt /home/project/airflow/dags/capstone/
        
        # 5. Nettoyer
        cd /
        rm -rf "$TMP_DIR"
        
        # 6. VÃ©rification
        if [ -f "/home/project/airflow/dags/capstone/extracted_data.txt" ]; then
            echo "âœ… EXTRACTION RÃ‰USSIE"
            echo "ðŸ“Š Lignes extraites: $(wc -l < /home/project/airflow/dags/capstone/extracted_data.txt)"
            echo "ðŸ“ Extrait:"
            head -3 /home/project/airflow/dags/capstone/extracted_data.txt
        else
            echo "âŒ Ã‰CHEC"
            exit 1
        fi
    """,
    dag=dag
)

# TÃ¢che de transformation - VERSION GARANTIE
transform_data = BashOperator(
    task_id='transform_data',
    bash_command="""
        # === UTILISATION DE /tmp POUR LES PERMISSIONS ===
        TMP_DIR="/tmp/airflow_transform_$$"
        mkdir -p "$TMP_DIR"
        cd "$TMP_DIR"
        
        # Copier le fichier source
        cp /home/project/airflow/dags/capstone/extracted_data.txt .
        
        # Filtrer
        grep -v "198.46.149.143" extracted_data.txt > transformed_data.txt
        
        # Copier le rÃ©sultat
        cp transformed_data.txt /home/project/airflow/dags/capstone/
        
        # Nettoyer et vÃ©rifier
        cd /
        rm -rf "$TMP_DIR"
        
        echo "âœ… TRANSFORMATION RÃ‰USSIE"
        echo "ðŸ“Š Avant: $(wc -l < /home/project/airflow/dags/capstone/extracted_data.txt)"
        echo "ðŸ“Š AprÃ¨s: $(wc -l < /home/project/airflow/dags/capstone/transformed_data.txt)"
    """,
    dag=dag
)

# TÃ¢che de chargement - VERSION GARANTIE
load_data = BashOperator(
    task_id='load_data',
    bash_command="""
        # === CRÃ‰ATION DIRECTE DANS LE RÃ‰PERTOIRE ===
        cd /home/project/airflow/dags/capstone
        
        # VÃ©rifier que le fichier existe
        if [ ! -f "transformed_data.txt" ]; then
            echo "âŒ Fichier transformed_data.txt manquant"
            ls -la
            exit 1
        fi
        
        # CrÃ©er l'archive
        tar -cvf weblog.tar transformed_data.txt
        
        echo "âœ… CHARGEMENT RÃ‰USSI"
        echo "ðŸ“¦ Archive crÃ©Ã©e: weblog.tar"
        echo "ðŸ“ Taille: $(ls -lh weblog.tar | awk '{print $5}')"
        echo "âœ… TÃ‚CHES TERMINÃ‰ES AVEC SUCCÃˆS"
    """,
    dag=dag
)

# DÃ©finir l'ordre d'exÃ©cution
extract_data >> transform_data >> load_data